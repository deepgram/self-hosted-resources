# Deepgram Self-Hosted - Airgapped Deployment (AWS EKS)
#
# This configuration deploys Deepgram in an airgapped environment where components
# cannot reach license.deepgram.com. The Billing container acts as the license terminal.
#
# For more information, see samples/airgapped.md and samples/README.md
# ---
global:
  # pullSecretRef should refer to a K8s secret that
  # must be created prior to installing this Chart.
  # Consult the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/) for best practices on configuring Secrets for use in your cluster.
  #
  # You can create a secret for your image pull credentials
  # with the following commands:
  # ```bash
  # docker login quay.io
  # kubectl create secret docker-registry dg-regcred \
  #   --docker-server=quay.io \
  #   --docker-username='QUAY_DG_USER' \
  #   --docker-password='QUAY_DG_PASSWORD'
  # ```
  pullSecretRef: "dg-regcred"

  # deepgramSecretRef - API key for self-hosted deployments (not used in airgapped mode)

  # deepgramLicenseSecretRef should refer to a K8s secret that
  # must be created prior to installing this Chart for airgapped deployments.
  # Consult the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/configuration/secret/) for best practices on configuring Secrets for use in your cluster.
  #
  # You can create a secret for your Deepgram license key
  # with the following command:
  # ```bash
  # kubectl create secret generic dg-license-key --from-literal=DEEPGRAM_LICENSE_KEY='<license-key>'
  # ```
  deepgramLicenseSecretRef: "dg-license-key"

scaling:
  replicas:
    api: 1
    engine: 1
  auto:
    # Can toggle to true to enable autoscaling. Make sure to set a value for one of the available metrics
    enabled: false
    engine:
      metrics:
        speechToText:
          batch:
            requestsPerPod: # Discuss a reasonable value with your Deepgram Account Representative
          streaming:
            requestsPerPod: # Discuss a reasonable value with your Deepgram Account Representative
        textToSpeech:
          batch:
            requestsPerPod: # Discuss a reasonable value with your Deepgram Account Representative
        # Discuss a reasoanble value with your Deepgram Account Representative
        # Must also set engine.concurrencyLimit.activeRequests if using request ratio for autoscaling
        requestCapacityRatio:

agent:
  enabled: false

api:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: k8s.deepgram.com/node-type
                operator: In
                values:
                  - api
  resources:
    requests:
      memory: "4Gi"
      cpu: "2000m"
    limits:
      memory: "8Gi"
      cpu: "4000m"
  
  # -- Service configuration for the API external service
  # Uncomment and modify the service type as needed for your deployment
  # service:
  #   # Options: ClusterIP (default), NodePort, LoadBalancer
  #   type: LoadBalancer
  #   # Add annotations when using LoadBalancer type (e.g., for AWS ELB configuration)
  #   annotations:
  #     service.beta.kubernetes.io/aws-load-balancer-type: "nlb"
  #     service.beta.kubernetes.io/aws-load-balancer-scheme: "internet-facing"
  #   # Restrict access to specific IP ranges (optional)
  #   loadBalancerSourceRanges:
  #     - "10.0.0.0/8"      # Allow access from private networks
  #     - "192.168.1.0/24"  # Allow access from specific subnet
  #   # External traffic policy: Cluster (default) or Local
  #   externalTrafficPolicy: "Local"  # Preserve source IP and reduce hops

  # -- Custom TOML sections can be added here to extend api.toml
  # customToml: |
  #   [custom_section]

engine:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: k8s.deepgram.com/node-type
                operator: In
                values:
                  - engine
  resources:
    requests:
      memory: "28Gi"
      cpu: "6000m"
      gpu: 1
    limits:
      memory: "40Gi"
      cpu: "8000m"
      gpu: 1
  # Discuss a reasonable value with your Deepgram Account Representative
  # If not using autoscaling, can be left empty, but must be set if using
  # autoscaling with scaling.auto.engine.metrics.requestCapacityRatio
  concurrencyLimit:
    activeRequests:

  # -- Custom TOML sections can be added here to extend engine.toml
  # customToml: |
  #   # Preload models on engine startup for faster initial requests
  #   # See https://deepgram.gitbook.io/help-center/how-can-i-pre-load-models-to-reduce-cold-start-latency
  #   [preload_models]
  #   models = [
  #     # Example model preload configuration:
  #     # { model = "general-nova-3", version = "2025-09-05.12808", language = "multi", format = false }
  #   ]

  modelManager:
    volumes:
      aws:
        efs:
          enabled: true
          fileSystemId: fs-xxxxxxxxxxxxxxxx # Replace with your EFS ID
          namePrefix: dg-models
    models:
      add:
        - https://link-to-model-1.dg # Replace these links with those provided to you
        - https://link-to-model-2.dg #   by your Deepgram Account Representative.
        - https://link-to-model-3.dg
        - ...
      remove:
        # - https://link-to-old-model-1.dg # Replace these with identifiers for any models already present
        # - https://link-to-old-model-2.dg #   in the EFS that you'd like removed. For a new installation,
        # - name-of-old-model-3.dg #   this will likely be empty.
        # - ...

billing:
  enabled: true
  replicas: 1
  
  # licenseFile should refer to a K8s secret containing your Deepgram license file.
  # The license file is a 1-line JSON file provided by Deepgram.
  #
  # You can create a secret for your license file with the following command:
  # ```bash
  # kubectl create secret generic dg-license-file --from-file=license.dg=/path/to/license.dg
  # ```
  licenseFile:
    secretRef: "dg-license-file"
    secretKey: "license.dg"
  
  journal:
    size: "1Gi"
    storageClass: gp2

  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: k8s.deepgram.com/node-type
                operator: In
                values:
                  - billing

licenseProxy:
  enabled: true
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: k8s.deepgram.com/node-type
                operator: In
                values:
                  - license-proxy

cluster-autoscaler:
  enabled: true
  rbac:
    serviceAccount:
      name: "cluster-autoscaler-sa"
      annotations:
        # Replace with the AWS Role ARN configured for the Cluster Autoscaler
        eks.amazonaws.com/role-arn: "arn:aws:iam::000000000000:role/MyRoleName"
  autoDiscovery:
    clusterName: "deepgram-self-hosted-cluster"
  awsRegion: "us-west-2"

gpu-operator:
  enabled: true

  # If using EKS accelerated AMIs based on Amazon Linux 2023 (default with Deepgram guides as of Oct 2024),
  # driver and toolkit come bundled with the AMI.
  #
  # If using other AMIs, such as Ubuntu, you should re-enable the driver and toolkit below.
  driver:
    enabled: false
  toolkit:
    enabled: false
